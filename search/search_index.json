{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GeoAI-Retail GeoAI-Retail is an opinionated analysis template striving to streamline and promote use of best practices for projects combining Geography and Artificial Intelligence for retail through a logical, reasonably standardized, and flexible project structure. A high level overview of the methods implemented in GeoAI-Retail is discussed in the Customer-Centric Analysis StoryMap (open full size) . GeoAI-Retail is an adaptation of GeoAI-Cookiecutter tailored for retail analysis workflows. GeoAI-Cookiecutter grew out of a need within the Advanced Analytics team at Esri to streamline project bootstrapping, encourage innovation, increase repeatability, encourage documentation, and encourage best practices based on strong opinions (best practices) . GeoAI-Retail implements these opinions with additional tight integration to the Esri Business Analyst extension capabilities heavily relying functionality from the BA-Tools Python package . This enables a data driven approach to model the relationship between who and where customers are, and customers' relationships to physical store locations using artificial intelligence. Why use this project structure? Data Science and GIS are both about providing actionable insights . The visual results communicating actionable insights, maps, graphs and infographics are typically what first come to mind. Not surprisingly, these information products frequently get the most attention. However, these visualizations merely communicate results of complex analysis, and the path toward unearthing these insights requires creative exploration coupled with repeatability. Actionable insights for decision making requires structure not impeding creative innovation. The best analyses are frequently the result of incredibly scattershot experimentation. This requires the ability to quickly test seemingly harebrained approaches. While most do not work, the ones that do, this is where innovation happens and genius is discovered. Once discovered, for the discovery to be useful, it must be repeatable. While creativity and repeatability may initially first feel at odds, a standardized project structure used according to best practices not only facilitates creativity, but also enables repeatability. This, in turn, increases the ability to iterate more quickly, discover more insights, and generally become more productive. Not only will your own work benefit, this benefits your colleagues as well. Others Will Thank You Templating tools started with web development. Pick a framework. Ember.js, Angular, Django, Rails - every single one has a project generator scaffolding out a project according to conventions and best practices. This speeds up starting a new project and makes collaboration much easier. Anybody else looking at the project will know where to look for resources, and since resources are logically organized, even if they are not familiar with the exact structure, can logically deduce where to find things. Thus, utilizing a standard template facilitates collaboration, repeatability, and confidence in your work. While applicable across disciplines, all of these become especially useful for projects part of research being submitted for peer review. You Will Thank You Repeatability not only applies to your colleagues being able to repeat your work, it applies even more to the person who must repeat your work most often, you . Spaghetti code with multiple steps, and experimental iterations in different notebooks is impossible to decipher two weeks, two months, two years, or all too frequently, even two days later. Revisiting previous analysis in disorganized projects takes considerable time, and detracts from working on new and innovative projects. Disorganization is frustrating and dramatically adversely affects your productivity. Good project structure encourages best practices, it makes them easy, so you can come back to your previous work easily and be productive! Nothing is Binding \"A foolish consistency is the hobgoblin of little minds\" \u2014 Ralph Waldo Emerson (and PEP 8! ) GeoAI-Retail is a starting point for projects, but is not absolute. If details do not work, change them in your project based on your needs, but be consistent . After all, this is how GeoAI-Retail came into existence! Cookiecutter Data Science is an excellent template for data science work, but did not meet all the needs for marrying Geographic and Artificial Intelligence analysis workflows. We extended and modified this template to achieve consistency addressing our additional needs. Even within a project, if something does not work, simply follow the path that does work, but do it with consistency . This way, for yourself and others looking at your work, your workflows and code will make sense. Consistency within a project is more important. Consistency within one module or function is the most important. ... However, know when to be inconsistent -- sometimes style guide recommendations just aren't applicable. When in doubt, use your best judgment. Look at other examples and decide what looks best. And don't hesitate to ask! \u2014 ( PEP 8! ) Asking is important! Learn from peers and colleagues. Getting Started GeoAI-Retail provides a template for starting projects taking advantage of the combined capabilities of ArcGIS Pro with the Business Analyst Extension , Esri Cloud GIS ( ArcGIS Enterprise or ArcGIS Online ), and the broad ecosystem of Machine Learning technologies using Python in a Conda environment. Requirements ArcGIS Pro 2.4 or greater (Python 3.6 and Conda come with it) Cookiecutter >= 1.4.0 Cookiecutter is not installed by default, but can easily be installed by opening the Python Command Prompt by going to Start > Programs > ArcGIS > Python Command Prompt. This opens a command prompt with the default Conda environment for ArcGIS Pro activated. This environment is named arcgispro-py3 , and is in parentheses prefixing the normal command prompt on the left side. This default environment should not be modified . Rather, copy and modify the copied environment. First, start by copying the environment. If following the convention in these code snippets, your new environment will be named arcgis . > conda create --name arcgis --clone arcgispro-py3 Next, activate this new environment. > activate arcgis Finally, install Cookiecutter. > conda install -c conda-forge cookiecutter -y Starting a New Project Now, once you have Cookiecutter installed in an environment, you can use the GeoAI-Retail template to quickly start a new project. > cookiecutter https://github.com/knu2xs/geoai-retail Once you answer all the questions, the new project is now created as a new directory in the current working directory. Assuming your project name is sik-pro , your new Conda environment is going to be named sik_pro . Hence, to create this environment, run the following. > cd sik-pro > make env This creates the environment and gets it ready for use. The following prompt will indicate this new environment is active by displaying the new environment name in parenthesis to the left of the command prompt. You are ready to get to work. The video below walks through not only the process of making a new project, but also the process of syncing data with Azure using the configured keys optionally set during project setup. It is much easier to see if you view it full screen. Project Organization and Structure The GeoAI-Retail template provides a structure for project resources, marrying data science directory structure with the functionality of ArcGIS Pro. This primarily means organizing the project following most of the best practices and conventions from Cookiecutter Data Science, and adapting ArcGIS Pro to easily work within this paradigm. Directory Structure \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` \u251c\u2500\u2500 make.bat <- Windows batch file with commands like `make data` \u251c\u2500\u2500 setup.py <- Setup script for the library ({{ cookiecutter.support_library }}) \u251c\u2500\u2500 .env <- Any environment variables here - created as part of project creation, \u2502 but NOT syncronized with git repo for project. \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 arcgis <- Root location for ArcGIS Pro project created as part of \u2502 \u2502 data science project creation. \u2502 \u251c\u2500\u2500 {{ cookiecutter.project_name }}.aprx <- ArcGIS Pro project. \u2502 \u251c\u2500\u2500 {{ cookiecutter.project_name }}.tbx <- ArcGIS Pro toolbox associated with the project. \u2502 \u2514\u2500\u2500 GeoAI-Tools.tbx<- Tools streamlining the process of project setup tying into Esri Business Analyst data. \u251c\u2500\u2500 scripts <- Put scripts to run things here. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u2502 \u2514\u2500\u2500 interim.gdb \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2502 \u2514\u2500\u2500 processed.gdb \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u2502 \u2514\u2500\u2500 raw.gdb \u251c\u2500\u2500 docs <- A default Sphinx project; see sphinx-doc.org for details \u251c\u2500\u2500 models <- Trained and serialized models, model predictions, or model summaries \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a 2 digits (for ordering), \u2502 \u2502 descriptive name. e.g.: 01_exploratory_analysis.ipynb \u2502 \u2514\u2500\u2500 notebook_template.ipynb \u251c\u2500\u2500 references <- Data dictionaries, manuals, and all other explanatory materials. \u251c\u2500\u2500 reports <- Generated analysis as HTML, PDF, LaTeX, etc. \u2502 \u2514\u2500\u2500 figures <- Generated graphics and figures to be used in reporting \u251c\u2500\u2500 environment.yml <- The requirements file for reproducing the analysis environment. This \u2502 is generated by running `conda env export > environment.yml` or \u2502 `make env_export`. \u2514\u2500\u2500 src <- Source code for use in this project. \u2514\u2500\u2500 {{ cookiecutter.support_library }} <- Library containing the bulk of code used in this project. ArcGIS Pro Project (aprx) Configuration While the ArcGIS Pro project is located in the ./arcgis directory, the data is located in another directory. To make life easier, the three file geodatabases, interim.gdb, processed.gdb and raw.gdb are all already registered. Additionally, the arcgis, data, and notebooks directories are all also registered in the Pro project. This makes it easy to access useful resources from ArcGIS Pro. NOTE: The notebooks directory is included due to support for Jupyter Notebooks introduced with ArcGIS Pro 2.5. If using a version below 2.5, while this directory still will be visible, it will appear as if nothing is in the directory when viewed from the Catalog Tree in ArcGIS Pro. This is due to ArcGIS Pro only displaying recognized file types, and Notebook files ( .ipynb ) are not recognized until version 2.5. Opinions Opinions guide best practices, but are still only opinions. However, following the best practices and conventions laid out by these opinions will go a long ways towards achieving repeatability, facilitating collaboration and increasing both your and your colleagues confidence in your work. In the case of our team, these opinions embody team practices and Esri company policies. Autonomous Environment The available machine learning and deep learning technologies available in Python is staggering. Fortunately, a large number of them are available simply as installable pip or conda packages. A very large part of reproducibility is being able to reproduce the environment. GeoAI-Retail facilitates this process by streamlining the process of creating a full functioning new Conda environment for a new project. To create a project preflighted with a few useful packages, simply ensure you are in the root of the project and type the following into the command line. > make env If you add more packages to this environment, to ensure repeatability, simply capture the packages in your environment by using the following command. > conda env export > environment.yml This will replace the original environment.yml . This enables recreating the modified project environment on a new machine as simple as again using the make env command. CAUTION: Simply using the normal Conda method of creating a new environment from the environment.yml file will not work . The new environment will not have arcpy available. To maintain the ability to use arcpy , the arcgispro-py3 environment must be cloned, and then the new environment must be updated using the environment.yml file as a template. This is what the make env command automatically takes care of. Notebooks are for Exploration and Communication Jupyter Notebooks are for exploration, experimentation and communication. Notebooks are NOT for data production and analysis reproduction. Notebooks have revolutionized data exploration and experimentation with code, and for this reason, are a favorite tool for exploring new ideas. For this reason, they should be used frequently to explore workflows. Notebooks are also tremendously valuable tool for highlighting analysis innovation and visualizing results dynamically. Frequently notebooks can be an integral part of communicating how analysis is accomplished and performed. However, the notebooks should succinctly highlight unique analysis aspects, not show the entire beginning to end process. Raw Data Immutability Immutable adjective - that cannot be changed; that will never change Synonym - unchangeable Leaving original data unchanged, and building an automated data transformation pipeline dramatically streamlines repeatability. This frequently is even to the extent of data format. This makes it much easier to revisit and revise results with new data as it becomes available. This is the explicit purpose of the ./data/raw directory. Put your untouched raw data in this directory, and develop the pipeline to transform this data into a usable form, placing this usable data into the ./data/interim directory. Further, to repeat the work, the data does not necessarily need to be in source control. By default, due to both this reason, and the common limitation of GitHub to store large files, the ./data directory is excluded from synchronization with source control in the .gitignore file. As a supplementary option for enabling access to the data, GeoAI-Retail includes the capability to upload and download data using Azure Blob Storage . Analysis is a DAG A directed acyclic graph (DAG) is a finite directed graph with no directed cycles. Frequently data preparation is a time consuming process, and especially in the early stages of development, involves a lot of trial and error. For this reason each step in the data preparation process should cache results to the interim data directory or file geodatabase, and following steps should check for this intermediate data. This way if intermediate data has already been created, the data preparation pipeline does not need to start over from the beginning every time. This is the structure of SciKit Learn Transformers , both out of the box and custom transformers, assembled into a data transformation Pipeline . This is useful both in development of the data pipeline and even in production. During development, this enables iterative experimentation to continue moving forward. Once in production, this enables troubleshooting where the problem in the pipeline occurred as opposed to having to start the entire process over just to find the one place in the code causing the issue. Keep Secrets Out of Version Control Sharing your work is highly encouraged, but sharing sensitive information definitely is not . Almost without exception, compromising credentials or tokens granting access to company or organization resources can quickly compromise your position with the organization. Please do not do this. To protect access to information, never put passwords or other sensitive information into any files synchronized with version control. GeoAI-Retail implements the convention of Cookiecutter Data Science by keeping this sensitive information in a special file excluded from version control, and provides a method of easily accessing these values from anywhere in the project. Special File .env The .env file is created by default for storing sensitive information when the project is created, but excluded from version control in the .gitignore file. If a project is downloaded from version control, typically GitHub, this file will be missing. It will either need to be procured through another channel, or simply manually recreated. By default, on project creation, the .env file will look similar to this. AZURE_STORAGE_ACCOUNT_NAME = azureaccountname AZURE_STORAGE_ACCOUNT_KEY = longcrypticcharacterstring ESRI_GIS_URL = https://myorgname.maps.arcgis.com ESRI_GIS_USERNAME = my_cloud_gis_username ESRI_GIS_PASSWORD = R3@!!y_H@rd_T0_Gu3$$_@nd_H@rd3r_T0_R3m3mb3r_P@$$w0rd Load Variables Using Dotenv Among the packages included in the default environment.yml file is python-dotenv . This package enables loading the variables so they are accessible using os.getenv . For instance, to be able to instantiate the GIS object, using variables stored in the .env file, the following can be used. from arcgis.gis import GIS from dotenv import load_dotenv, find_dotenv # find .env automagically by walking up directories until found and load entries as env vars load_dotenv(find_dotenv()) # use environment variables to instantiate the GIS object gis = GIS( url=os.getenv('ESRI_GIS_URL'), username=os.getenv('ESRI_GIS_USERNAME'), password=os.getenv('ESRI_GIS_PASSWORD') ) Thank You! GeoAI-Retail is built on the shoulders of giants. It is merely an evolution of existing work by DrivenData and Cookiecutter to suit the needs we have working at the intersection of geography and artificial intelligence to solve retail challenges. We owe a huge debt of gratitude to both. DrivenData Huge thank you to DrivenData for creating the outstanding Cookiecutter Data Science template we used as the starting point for GeoAI-Retail. Cookiecutter Data Science provided the framework for initially addressing many of our team's internal collaboration challenges, and evolved into what you see here. DrivenData, we could not have done it without you! Thank you! Cookiecutter Neither GeoAI-Retail nor Cookiecutter Data Science would be possible without the outstanding open source project Cookiecutter. Licensing Copyright 2020 Esri Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. A copy of the license is available directly from the Apache foundation .","title":"Home"},{"location":"#geoai-retail","text":"GeoAI-Retail is an opinionated analysis template striving to streamline and promote use of best practices for projects combining Geography and Artificial Intelligence for retail through a logical, reasonably standardized, and flexible project structure. A high level overview of the methods implemented in GeoAI-Retail is discussed in the Customer-Centric Analysis StoryMap (open full size) . GeoAI-Retail is an adaptation of GeoAI-Cookiecutter tailored for retail analysis workflows. GeoAI-Cookiecutter grew out of a need within the Advanced Analytics team at Esri to streamline project bootstrapping, encourage innovation, increase repeatability, encourage documentation, and encourage best practices based on strong opinions (best practices) . GeoAI-Retail implements these opinions with additional tight integration to the Esri Business Analyst extension capabilities heavily relying functionality from the BA-Tools Python package . This enables a data driven approach to model the relationship between who and where customers are, and customers' relationships to physical store locations using artificial intelligence.","title":"GeoAI-Retail"},{"location":"#why-use-this-project-structure","text":"Data Science and GIS are both about providing actionable insights . The visual results communicating actionable insights, maps, graphs and infographics are typically what first come to mind. Not surprisingly, these information products frequently get the most attention. However, these visualizations merely communicate results of complex analysis, and the path toward unearthing these insights requires creative exploration coupled with repeatability. Actionable insights for decision making requires structure not impeding creative innovation. The best analyses are frequently the result of incredibly scattershot experimentation. This requires the ability to quickly test seemingly harebrained approaches. While most do not work, the ones that do, this is where innovation happens and genius is discovered. Once discovered, for the discovery to be useful, it must be repeatable. While creativity and repeatability may initially first feel at odds, a standardized project structure used according to best practices not only facilitates creativity, but also enables repeatability. This, in turn, increases the ability to iterate more quickly, discover more insights, and generally become more productive. Not only will your own work benefit, this benefits your colleagues as well.","title":"Why use this project structure?"},{"location":"#others-will-thank-you","text":"Templating tools started with web development. Pick a framework. Ember.js, Angular, Django, Rails - every single one has a project generator scaffolding out a project according to conventions and best practices. This speeds up starting a new project and makes collaboration much easier. Anybody else looking at the project will know where to look for resources, and since resources are logically organized, even if they are not familiar with the exact structure, can logically deduce where to find things. Thus, utilizing a standard template facilitates collaboration, repeatability, and confidence in your work. While applicable across disciplines, all of these become especially useful for projects part of research being submitted for peer review.","title":"Others Will Thank You"},{"location":"#you-will-thank-you","text":"Repeatability not only applies to your colleagues being able to repeat your work, it applies even more to the person who must repeat your work most often, you . Spaghetti code with multiple steps, and experimental iterations in different notebooks is impossible to decipher two weeks, two months, two years, or all too frequently, even two days later. Revisiting previous analysis in disorganized projects takes considerable time, and detracts from working on new and innovative projects. Disorganization is frustrating and dramatically adversely affects your productivity. Good project structure encourages best practices, it makes them easy, so you can come back to your previous work easily and be productive!","title":"You Will Thank You"},{"location":"#nothing-is-binding","text":"\"A foolish consistency is the hobgoblin of little minds\" \u2014 Ralph Waldo Emerson (and PEP 8! ) GeoAI-Retail is a starting point for projects, but is not absolute. If details do not work, change them in your project based on your needs, but be consistent . After all, this is how GeoAI-Retail came into existence! Cookiecutter Data Science is an excellent template for data science work, but did not meet all the needs for marrying Geographic and Artificial Intelligence analysis workflows. We extended and modified this template to achieve consistency addressing our additional needs. Even within a project, if something does not work, simply follow the path that does work, but do it with consistency . This way, for yourself and others looking at your work, your workflows and code will make sense. Consistency within a project is more important. Consistency within one module or function is the most important. ... However, know when to be inconsistent -- sometimes style guide recommendations just aren't applicable. When in doubt, use your best judgment. Look at other examples and decide what looks best. And don't hesitate to ask! \u2014 ( PEP 8! ) Asking is important! Learn from peers and colleagues.","title":"Nothing is Binding"},{"location":"#getting-started","text":"GeoAI-Retail provides a template for starting projects taking advantage of the combined capabilities of ArcGIS Pro with the Business Analyst Extension , Esri Cloud GIS ( ArcGIS Enterprise or ArcGIS Online ), and the broad ecosystem of Machine Learning technologies using Python in a Conda environment.","title":"Getting Started"},{"location":"#requirements","text":"ArcGIS Pro 2.4 or greater (Python 3.6 and Conda come with it) Cookiecutter >= 1.4.0 Cookiecutter is not installed by default, but can easily be installed by opening the Python Command Prompt by going to Start > Programs > ArcGIS > Python Command Prompt. This opens a command prompt with the default Conda environment for ArcGIS Pro activated. This environment is named arcgispro-py3 , and is in parentheses prefixing the normal command prompt on the left side. This default environment should not be modified . Rather, copy and modify the copied environment. First, start by copying the environment. If following the convention in these code snippets, your new environment will be named arcgis . > conda create --name arcgis --clone arcgispro-py3 Next, activate this new environment. > activate arcgis Finally, install Cookiecutter. > conda install -c conda-forge cookiecutter -y","title":"Requirements"},{"location":"#starting-a-new-project","text":"Now, once you have Cookiecutter installed in an environment, you can use the GeoAI-Retail template to quickly start a new project. > cookiecutter https://github.com/knu2xs/geoai-retail Once you answer all the questions, the new project is now created as a new directory in the current working directory. Assuming your project name is sik-pro , your new Conda environment is going to be named sik_pro . Hence, to create this environment, run the following. > cd sik-pro > make env This creates the environment and gets it ready for use. The following prompt will indicate this new environment is active by displaying the new environment name in parenthesis to the left of the command prompt. You are ready to get to work. The video below walks through not only the process of making a new project, but also the process of syncing data with Azure using the configured keys optionally set during project setup. It is much easier to see if you view it full screen.","title":"Starting a New Project"},{"location":"#project-organization-and-structure","text":"The GeoAI-Retail template provides a structure for project resources, marrying data science directory structure with the functionality of ArcGIS Pro. This primarily means organizing the project following most of the best practices and conventions from Cookiecutter Data Science, and adapting ArcGIS Pro to easily work within this paradigm.","title":"Project Organization and Structure"},{"location":"#directory-structure","text":"\u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` \u251c\u2500\u2500 make.bat <- Windows batch file with commands like `make data` \u251c\u2500\u2500 setup.py <- Setup script for the library ({{ cookiecutter.support_library }}) \u251c\u2500\u2500 .env <- Any environment variables here - created as part of project creation, \u2502 but NOT syncronized with git repo for project. \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 arcgis <- Root location for ArcGIS Pro project created as part of \u2502 \u2502 data science project creation. \u2502 \u251c\u2500\u2500 {{ cookiecutter.project_name }}.aprx <- ArcGIS Pro project. \u2502 \u251c\u2500\u2500 {{ cookiecutter.project_name }}.tbx <- ArcGIS Pro toolbox associated with the project. \u2502 \u2514\u2500\u2500 GeoAI-Tools.tbx<- Tools streamlining the process of project setup tying into Esri Business Analyst data. \u251c\u2500\u2500 scripts <- Put scripts to run things here. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u2502 \u2514\u2500\u2500 interim.gdb \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2502 \u2514\u2500\u2500 processed.gdb \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u2502 \u2514\u2500\u2500 raw.gdb \u251c\u2500\u2500 docs <- A default Sphinx project; see sphinx-doc.org for details \u251c\u2500\u2500 models <- Trained and serialized models, model predictions, or model summaries \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a 2 digits (for ordering), \u2502 \u2502 descriptive name. e.g.: 01_exploratory_analysis.ipynb \u2502 \u2514\u2500\u2500 notebook_template.ipynb \u251c\u2500\u2500 references <- Data dictionaries, manuals, and all other explanatory materials. \u251c\u2500\u2500 reports <- Generated analysis as HTML, PDF, LaTeX, etc. \u2502 \u2514\u2500\u2500 figures <- Generated graphics and figures to be used in reporting \u251c\u2500\u2500 environment.yml <- The requirements file for reproducing the analysis environment. This \u2502 is generated by running `conda env export > environment.yml` or \u2502 `make env_export`. \u2514\u2500\u2500 src <- Source code for use in this project. \u2514\u2500\u2500 {{ cookiecutter.support_library }} <- Library containing the bulk of code used in this project.","title":"Directory Structure"},{"location":"#arcgis-pro-project-aprx-configuration","text":"While the ArcGIS Pro project is located in the ./arcgis directory, the data is located in another directory. To make life easier, the three file geodatabases, interim.gdb, processed.gdb and raw.gdb are all already registered. Additionally, the arcgis, data, and notebooks directories are all also registered in the Pro project. This makes it easy to access useful resources from ArcGIS Pro. NOTE: The notebooks directory is included due to support for Jupyter Notebooks introduced with ArcGIS Pro 2.5. If using a version below 2.5, while this directory still will be visible, it will appear as if nothing is in the directory when viewed from the Catalog Tree in ArcGIS Pro. This is due to ArcGIS Pro only displaying recognized file types, and Notebook files ( .ipynb ) are not recognized until version 2.5.","title":"ArcGIS Pro Project (aprx) Configuration"},{"location":"#opinions","text":"Opinions guide best practices, but are still only opinions. However, following the best practices and conventions laid out by these opinions will go a long ways towards achieving repeatability, facilitating collaboration and increasing both your and your colleagues confidence in your work. In the case of our team, these opinions embody team practices and Esri company policies.","title":"Opinions"},{"location":"#autonomous-environment","text":"The available machine learning and deep learning technologies available in Python is staggering. Fortunately, a large number of them are available simply as installable pip or conda packages. A very large part of reproducibility is being able to reproduce the environment. GeoAI-Retail facilitates this process by streamlining the process of creating a full functioning new Conda environment for a new project. To create a project preflighted with a few useful packages, simply ensure you are in the root of the project and type the following into the command line. > make env If you add more packages to this environment, to ensure repeatability, simply capture the packages in your environment by using the following command. > conda env export > environment.yml This will replace the original environment.yml . This enables recreating the modified project environment on a new machine as simple as again using the make env command. CAUTION: Simply using the normal Conda method of creating a new environment from the environment.yml file will not work . The new environment will not have arcpy available. To maintain the ability to use arcpy , the arcgispro-py3 environment must be cloned, and then the new environment must be updated using the environment.yml file as a template. This is what the make env command automatically takes care of.","title":"Autonomous Environment"},{"location":"#notebooks-are-for-exploration-and-communication","text":"Jupyter Notebooks are for exploration, experimentation and communication. Notebooks are NOT for data production and analysis reproduction. Notebooks have revolutionized data exploration and experimentation with code, and for this reason, are a favorite tool for exploring new ideas. For this reason, they should be used frequently to explore workflows. Notebooks are also tremendously valuable tool for highlighting analysis innovation and visualizing results dynamically. Frequently notebooks can be an integral part of communicating how analysis is accomplished and performed. However, the notebooks should succinctly highlight unique analysis aspects, not show the entire beginning to end process.","title":"Notebooks are for Exploration and Communication"},{"location":"#raw-data-immutability","text":"Immutable adjective - that cannot be changed; that will never change Synonym - unchangeable Leaving original data unchanged, and building an automated data transformation pipeline dramatically streamlines repeatability. This frequently is even to the extent of data format. This makes it much easier to revisit and revise results with new data as it becomes available. This is the explicit purpose of the ./data/raw directory. Put your untouched raw data in this directory, and develop the pipeline to transform this data into a usable form, placing this usable data into the ./data/interim directory. Further, to repeat the work, the data does not necessarily need to be in source control. By default, due to both this reason, and the common limitation of GitHub to store large files, the ./data directory is excluded from synchronization with source control in the .gitignore file. As a supplementary option for enabling access to the data, GeoAI-Retail includes the capability to upload and download data using Azure Blob Storage .","title":"Raw Data Immutability"},{"location":"#analysis-is-a-dag","text":"A directed acyclic graph (DAG) is a finite directed graph with no directed cycles. Frequently data preparation is a time consuming process, and especially in the early stages of development, involves a lot of trial and error. For this reason each step in the data preparation process should cache results to the interim data directory or file geodatabase, and following steps should check for this intermediate data. This way if intermediate data has already been created, the data preparation pipeline does not need to start over from the beginning every time. This is the structure of SciKit Learn Transformers , both out of the box and custom transformers, assembled into a data transformation Pipeline . This is useful both in development of the data pipeline and even in production. During development, this enables iterative experimentation to continue moving forward. Once in production, this enables troubleshooting where the problem in the pipeline occurred as opposed to having to start the entire process over just to find the one place in the code causing the issue.","title":"Analysis is a DAG"},{"location":"#keep-secrets-out-of-version-control","text":"Sharing your work is highly encouraged, but sharing sensitive information definitely is not . Almost without exception, compromising credentials or tokens granting access to company or organization resources can quickly compromise your position with the organization. Please do not do this. To protect access to information, never put passwords or other sensitive information into any files synchronized with version control. GeoAI-Retail implements the convention of Cookiecutter Data Science by keeping this sensitive information in a special file excluded from version control, and provides a method of easily accessing these values from anywhere in the project.","title":"Keep Secrets Out of Version Control"},{"location":"#special-file-env","text":"The .env file is created by default for storing sensitive information when the project is created, but excluded from version control in the .gitignore file. If a project is downloaded from version control, typically GitHub, this file will be missing. It will either need to be procured through another channel, or simply manually recreated. By default, on project creation, the .env file will look similar to this. AZURE_STORAGE_ACCOUNT_NAME = azureaccountname AZURE_STORAGE_ACCOUNT_KEY = longcrypticcharacterstring ESRI_GIS_URL = https://myorgname.maps.arcgis.com ESRI_GIS_USERNAME = my_cloud_gis_username ESRI_GIS_PASSWORD = R3@!!y_H@rd_T0_Gu3$$_@nd_H@rd3r_T0_R3m3mb3r_P@$$w0rd","title":"Special File .env"},{"location":"#load-variables-using-dotenv","text":"Among the packages included in the default environment.yml file is python-dotenv . This package enables loading the variables so they are accessible using os.getenv . For instance, to be able to instantiate the GIS object, using variables stored in the .env file, the following can be used. from arcgis.gis import GIS from dotenv import load_dotenv, find_dotenv # find .env automagically by walking up directories until found and load entries as env vars load_dotenv(find_dotenv()) # use environment variables to instantiate the GIS object gis = GIS( url=os.getenv('ESRI_GIS_URL'), username=os.getenv('ESRI_GIS_USERNAME'), password=os.getenv('ESRI_GIS_PASSWORD') )","title":"Load Variables Using Dotenv"},{"location":"#thank-you","text":"GeoAI-Retail is built on the shoulders of giants. It is merely an evolution of existing work by DrivenData and Cookiecutter to suit the needs we have working at the intersection of geography and artificial intelligence to solve retail challenges. We owe a huge debt of gratitude to both.","title":"Thank You!"},{"location":"#drivendata","text":"Huge thank you to DrivenData for creating the outstanding Cookiecutter Data Science template we used as the starting point for GeoAI-Retail. Cookiecutter Data Science provided the framework for initially addressing many of our team's internal collaboration challenges, and evolved into what you see here. DrivenData, we could not have done it without you! Thank you!","title":"DrivenData"},{"location":"#cookiecutter","text":"Neither GeoAI-Retail nor Cookiecutter Data Science would be possible without the outstanding open source project Cookiecutter.","title":"Cookiecutter"},{"location":"#licensing","text":"Copyright 2020 Esri Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. A copy of the license is available directly from the Apache foundation .","title":"Licensing"},{"location":"env_setup/","text":"Environment Setup The environment for running GeoAI-Retail must start with ArcGIS Pro with the Business Analyst Extension using local data. The Python environment installed with ArcGIS Pro is an instance of Conda, GeoAI-Retail extends the functionality through a few more capabilities available in this environment with additional packages. This involves creating a new environment and installing a few additional packages. Broadly, the steps to get up and running include the following. Install ArcGIS Pro with Business Analyst and local data Clone the ArcGIS Pro default environment, arcgispro-py3 Install the cookiecutter package into the new environment Create a new project using the GeoAI-Retail template Clone the Default Environment Python in ArcGIS Pro runs in a discrete Conda environment. This environment should never be modified. Rather, a copy should be made, and modifications made to this copied environment. Further, if you are familiar with creating environments using an environment.yml file, this does not work due to the way the bindings work for accessing the functionality offered by ArcGIS Pro in Python through the arcpy module. Copying the default environment retains this functionality. After installation, interacting with the Python Conda environment installed with ArcGIS Pro through the command line is accessed by going to Start > Programs > Python Command Prompt. With the prompt open, to the left of the normal path in the command prompt, you will see the name of the Conda environment in parentheses. If you have not changed this, it will be arcgispro-py3 . From this command prompt, you can begin by cloning the default Python environment using the following command. If following my convention, this new environment will be called arcgis . > conda create --name arcgis --clone arcgispro-py3 Next, activate the environment to begin working in it. > activate arcgis Now, you will see this new environment name to the left of the command prompt in parenthesis. NOTE: If you want this available in every command window, you can add Conda to your PATH environment variable. I typically do this when setting up a new machine by opening up a command prompt as administrator and using the following command. > setx path \"%PATH%;C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\Scripts\" Install Cookiecutter To use Cookiecutter templates, you first need to install Cookiecutter. This can be performed using the following command. conda install -c conda-forge cookiecutter Once installed, you can now use Cookiecutter templates to start new projects, including GeoAI-Retail. Try a New Project From here, you are ready to start a new project, and get to work. Try this by switching to a directory where you can create a new folder, ensuring the environment is active where you installed Cookiecutter ( arcgis if you followed my convention) and running the following command. > cookiecutter https://github.com/esri/geoai-retail You will be asked to answer a few questions about your new project. For now, accepting the defaults will work for testing. Then, just to ensure everything is ready to start working, switch into the newly created directory, and try creating the Conda environment for your new project using the following command. > make env This will create a Conda environment and install a few useful packages to get started working using GeoAI-Retail. If all of this works successfully, you are ready to dig in and start doing analysis using GeoAI-Retail!","title":"Environment Setup"},{"location":"env_setup/#environment-setup","text":"The environment for running GeoAI-Retail must start with ArcGIS Pro with the Business Analyst Extension using local data. The Python environment installed with ArcGIS Pro is an instance of Conda, GeoAI-Retail extends the functionality through a few more capabilities available in this environment with additional packages. This involves creating a new environment and installing a few additional packages. Broadly, the steps to get up and running include the following. Install ArcGIS Pro with Business Analyst and local data Clone the ArcGIS Pro default environment, arcgispro-py3 Install the cookiecutter package into the new environment Create a new project using the GeoAI-Retail template","title":"Environment Setup"},{"location":"env_setup/#clone-the-default-environment","text":"Python in ArcGIS Pro runs in a discrete Conda environment. This environment should never be modified. Rather, a copy should be made, and modifications made to this copied environment. Further, if you are familiar with creating environments using an environment.yml file, this does not work due to the way the bindings work for accessing the functionality offered by ArcGIS Pro in Python through the arcpy module. Copying the default environment retains this functionality. After installation, interacting with the Python Conda environment installed with ArcGIS Pro through the command line is accessed by going to Start > Programs > Python Command Prompt. With the prompt open, to the left of the normal path in the command prompt, you will see the name of the Conda environment in parentheses. If you have not changed this, it will be arcgispro-py3 . From this command prompt, you can begin by cloning the default Python environment using the following command. If following my convention, this new environment will be called arcgis . > conda create --name arcgis --clone arcgispro-py3 Next, activate the environment to begin working in it. > activate arcgis Now, you will see this new environment name to the left of the command prompt in parenthesis. NOTE: If you want this available in every command window, you can add Conda to your PATH environment variable. I typically do this when setting up a new machine by opening up a command prompt as administrator and using the following command. > setx path \"%PATH%;C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\Scripts\"","title":"Clone the Default Environment"},{"location":"env_setup/#install-cookiecutter","text":"To use Cookiecutter templates, you first need to install Cookiecutter. This can be performed using the following command. conda install -c conda-forge cookiecutter Once installed, you can now use Cookiecutter templates to start new projects, including GeoAI-Retail.","title":"Install Cookiecutter"},{"location":"env_setup/#try-a-new-project","text":"From here, you are ready to start a new project, and get to work. Try this by switching to a directory where you can create a new folder, ensuring the environment is active where you installed Cookiecutter ( arcgis if you followed my convention) and running the following command. > cookiecutter https://github.com/esri/geoai-retail You will be asked to answer a few questions about your new project. For now, accepting the defaults will work for testing. Then, just to ensure everything is ready to start working, switch into the newly created directory, and try creating the Conda environment for your new project using the following command. > make env This will create a Conda environment and install a few useful packages to get started working using GeoAI-Retail. If all of this works successfully, you are ready to dig in and start doing analysis using GeoAI-Retail!","title":"Try a New Project"},{"location":"make/","text":"Make - Using Make Commands There is a lot of functionality included in the GeoAI-Retail template. Instead of writing endless documentation detailing how to find and use all these resources, we have created a file, make.bat , containing shortcuts to accomplish a whole boatload of tasks. As with most all the functionality in this template, this came out of our own needs to streamline workflows, and not have to dig all around in the template to get boring and routing tasks accomplished. The commands available in make.bat fall into three general categories, data preprocessing, data management, and environment management. Data Preprocessing More than anything else, GeoAI-Retail is a geographic feature engineering engine to create quantitative factors for use in machine learning modeling. GeoAI-Retail can then be used to revise the features and perform inferencing using the models created from the original data. Fortunately, for inferencing only a small amount of feature engineering needs to be performed. > make data The initial step of preparing the data for analysis can take a decent amount of time. The heavy lifting is performed using a script, make_data.py . While this script can be run directly, to make life easier, you can invoke the script directly using the command make data . Data Management Although the code can be synchronized with version control, typically GitHub, datasets can be large, and frequently do not work well with version control. As a result, the data directory is excluded from version control in the .gitignore file, and can be saved to Azure Blob Storage. > make get_data This is particularly useful when collaborating on a project. After retrieving a project from version control, you can retrieve the data needed for the project using this command. The data will be downloaded from Azure Blob storage using credentials saved in the .env file and automatically extracted to the ./data directory. > make push_data This creates a zipped archive of the entire contents of the ./data directory, and pushes it to Azure Blob storage using credentials saved in the .env file. Environment Management Managing the Python Conda environment is dramatically streamlined using the commands contained in make.bat . Quite honestly, this is one of the single largest motivating factors for initially creating it. > make env This is the most commonly used command. This command creates a Conda environment using the name set up when originally creating the project. Due to some nuances of how Conda is configured with ArcGIS Pro, you cannot simply create a new environment directly from the environment.yml . Rather, you have to clone the default ArcGIS Pro Conda environment arcgispro-py3 and update the new environment using the environment.yml file. Additionally, if you like to use the mapping widget in Jupyter Lab, there are two additional steps. Hence, all of this is consolidated into one single step. > make env_activate Sometimes the environment name is a little long, and sometimes you cannot recall what it is. Either way, it does not matter. This command will activate the project environment created using the command above, so you can get to work! > make env_remove Multiple environments for projects quickly litter your computer. Hence, once finished with an environment for a project, this makes it easier to remove the environment from the machine.","title":"Make"},{"location":"make/#make-using-make-commands","text":"There is a lot of functionality included in the GeoAI-Retail template. Instead of writing endless documentation detailing how to find and use all these resources, we have created a file, make.bat , containing shortcuts to accomplish a whole boatload of tasks. As with most all the functionality in this template, this came out of our own needs to streamline workflows, and not have to dig all around in the template to get boring and routing tasks accomplished. The commands available in make.bat fall into three general categories, data preprocessing, data management, and environment management.","title":"Make - Using Make Commands"},{"location":"make/#data-preprocessing","text":"More than anything else, GeoAI-Retail is a geographic feature engineering engine to create quantitative factors for use in machine learning modeling. GeoAI-Retail can then be used to revise the features and perform inferencing using the models created from the original data. Fortunately, for inferencing only a small amount of feature engineering needs to be performed.","title":"Data Preprocessing"},{"location":"make/#gt-make-data","text":"The initial step of preparing the data for analysis can take a decent amount of time. The heavy lifting is performed using a script, make_data.py . While this script can be run directly, to make life easier, you can invoke the script directly using the command make data .","title":"&gt; make data"},{"location":"make/#data-management","text":"Although the code can be synchronized with version control, typically GitHub, datasets can be large, and frequently do not work well with version control. As a result, the data directory is excluded from version control in the .gitignore file, and can be saved to Azure Blob Storage.","title":"Data Management"},{"location":"make/#gt-make-get_data","text":"This is particularly useful when collaborating on a project. After retrieving a project from version control, you can retrieve the data needed for the project using this command. The data will be downloaded from Azure Blob storage using credentials saved in the .env file and automatically extracted to the ./data directory.","title":"&gt; make get_data"},{"location":"make/#gt-make-push_data","text":"This creates a zipped archive of the entire contents of the ./data directory, and pushes it to Azure Blob storage using credentials saved in the .env file.","title":"&gt; make push_data"},{"location":"make/#environment-management","text":"Managing the Python Conda environment is dramatically streamlined using the commands contained in make.bat . Quite honestly, this is one of the single largest motivating factors for initially creating it.","title":"Environment Management"},{"location":"make/#gt-make-env","text":"This is the most commonly used command. This command creates a Conda environment using the name set up when originally creating the project. Due to some nuances of how Conda is configured with ArcGIS Pro, you cannot simply create a new environment directly from the environment.yml . Rather, you have to clone the default ArcGIS Pro Conda environment arcgispro-py3 and update the new environment using the environment.yml file. Additionally, if you like to use the mapping widget in Jupyter Lab, there are two additional steps. Hence, all of this is consolidated into one single step.","title":"&gt; make env"},{"location":"make/#gt-make-env_activate","text":"Sometimes the environment name is a little long, and sometimes you cannot recall what it is. Either way, it does not matter. This command will activate the project environment created using the command above, so you can get to work!","title":"&gt; make env_activate"},{"location":"make/#gt-make-env_remove","text":"Multiple environments for projects quickly litter your computer. Hence, once finished with an environment for a project, this makes it easier to remove the environment from the machine.","title":"&gt; make env_remove"}]}